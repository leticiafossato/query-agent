{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query em Excel utilizando LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "import pandas as pd\n",
    "from langchain.agents import Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain_experimental.agents import create_pandas_dataframe_agent\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('dataset-vendas.xlsx')\n",
    "df = df[['Numero','Codigo do produto', 'Quantidade']]\n",
    "df = df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOllama\n",
    "# llm = ChatOllama(model=\"mistral\")\n",
    "# llm.invoke(\"O que é correlação?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resposta = agent.run(\"Qual foi o total de vendas por mês?\")\n",
    "# print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opção 1: Custom prompt + pandas interpreter\n",
    "\n",
    "Esta opção necessita de vários detalhes para a criação da consulta/tratativa pandas correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " df['Quantidade'].value_counts().idxmax()\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"pergunta\"],\n",
    "    template=\"\"\"\n",
    "Você é um especialista em Python. Um analista de dados fez a seguinte pergunta sobre um DataFrame chamado `df`.\n",
    "O dataframe tem as colunas: Quantidade, Numero e Codigo do produto. Não invente novas colunas, mas pode fazer tratativas e agrupamentos com as colunas existentes.\n",
    "Pergunta:\n",
    "\n",
    "{pergunta}\n",
    "\n",
    "Gere apenas o código Python (com pandas) necessário para responder. Não escreva explicações.\n",
    "Como saída, quero somente o código em Python, pois irei roda-lo diretamente em outra função. Não explique nada.\n",
    "Exemplo de saída: df['Quantidade'].value_counts().idxmax()\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4\")\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "pergunta = \"Qual o produto com maior quantidade?\"\n",
    "codigo_gerado = chain.run(pergunta)\n",
    "print(codigo_gerado)\n",
    "\n",
    "# Opcional: execute o código com segurança\n",
    "local_vars = {\"df\": df}\n",
    "# exec(limpar_codigo(codigo_gerado),{},local_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" df['Quantidade'].value_counts().idxmax()\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codigo_gerado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "codigo_gerado = re.sub('python','',codigo_gerado)\n",
    "codigo_gerado = re.sub('```','',codigo_gerado)\n",
    "codigo_gerado = re.sub('`','',codigo_gerado)\n",
    "codigo_gerado = re.sub('\\n','',codigo_gerado).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df['Quantidade'].value_counts().idxmax()\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codigo_gerado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(85)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = eval(codigo_gerado,{},local_vars) #eval ou exec, o exec não caputa o retorno\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra alternativa, ao invés de exec ou eval é o Python_REPL do Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opção 2:  LangChain Tools personalizados (como funções)\n",
    "\n",
    "Há mais controle dos cálculos executados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Você é um assistente que só pode usar as ferramentas listadas abaixo, referenciando elas pelo nome **sem parênteses**.\n",
    "Use exatamente o nome da ferramenta.\n",
    "\n",
    "Ferramentas disponíveis:\n",
    "- soma_por_categoria: Retorna a soma das vendas por categoria\n",
    "\n",
    "Quando precisar usar uma ferramenta, escreva apenas o nome dela sem parênteses.\n",
    "Pergunta: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"input\"], template=template)\n",
    "\n",
    "\n",
    "# Seu DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Categoria\": [\"A\", \"B\", \"A\", \"C\"],\n",
    "    \"Valor\": [10, 20, 15, 5]\n",
    "})\n",
    "\n",
    "# def soma_por_categoria():\n",
    "#     return df.groupby(\"Categoria\")[\"Valor\"].sum().to_string()\n",
    "\n",
    "def soma_por_categoria(tool_input: str) -> str:\n",
    "    # O parâmetro tool_input é o texto que o agente passa para a tool. [ignorar]\n",
    "    return df.groupby(\"Categoria\")[\"Valor\"].sum().to_string()\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"soma_por_categoria\", func=soma_por_categoria, description=\"Retorna a soma das vendas por categoria\"),\n",
    "]\n",
    "\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "#agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    agent_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m To find the sum of sales by category, I will use the `soma_por_categoria` tool.\n",
      "\n",
      "Action: soma_por_categoria\n",
      "Action Input: No input is needed for this function as it calculates the total sales by category from the existing data.\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mCategoria\n",
      "A    25\n",
      "B    20\n",
      "C     5\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: A soma das vendas por categoria é: A - 25, B - 20 e C - 5. A soma total é 50.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "resposta = agent.run(\"Qual a soma das vendas por categoria?\")\n",
    "\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A soma das vendas por categoria é: A - 25, B - 20 e C - 5. A soma total é 50.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opção 3 - create_pandas_dataframe_agent\n",
    "\n",
    "Esta opção é mais direta, mas não há muito controle sobre a criação do código python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('dataset-vendas.xlsx')\n",
    "df = df[['Numero','Codigo do produto', 'Quantidade']]\n",
    "df = df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Numero</th>\n",
       "      <th>Codigo do produto</th>\n",
       "      <th>Quantidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27192</td>\n",
       "      <td>1022450</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27227</td>\n",
       "      <td>1022450</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27235</td>\n",
       "      <td>1022450</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27250</td>\n",
       "      <td>1022450</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27254</td>\n",
       "      <td>1022450</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27265</td>\n",
       "      <td>1022450</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27266</td>\n",
       "      <td>1022450</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27269</td>\n",
       "      <td>1022450</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27281</td>\n",
       "      <td>1022450</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27283</td>\n",
       "      <td>1022450</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Numero  Codigo do produto  Quantidade\n",
       "0   27192            1022450          68\n",
       "1   27227            1022450          72\n",
       "2   27235            1022450          30\n",
       "3   27250            1022450          26\n",
       "4   27254            1022450          94\n",
       "5   27265            1022450          58\n",
       "6   27266            1022450          54\n",
       "7   27269            1022450          49\n",
       "8   27281            1022450          46\n",
       "9   27283            1022450          22"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: I need to find the row with the maximum quantity sold in the dataframe. I can do this by using the `df.loc` function which allows me to select rows based on some condition.\n",
      "Action: python_repl_ast\n",
      "Action Input: `print(df.loc[df['Quantidade'].idxmax()])`\u001b[0m\u001b[36;1m\u001b[1;3mNumero                 27254\n",
      "Codigo do produto    1022450\n",
      "Quantidade                94\n",
      "Name: 4, dtype: int64\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m I now know the final answer. The number with the highest quantity sold is 27254 with a quantity of 94.\n",
      "Final Answer: The number with the highest quantity sold is 27254 with a quantity of 94.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The number with the highest quantity sold is 27254 with a quantity of 94.\n"
     ]
    }
   ],
   "source": [
    "agent = create_pandas_dataframe_agent(llm, df, verbose=True, allow_dangerous_code=True)\n",
    "\n",
    "resposta = agent.run(\"Qual o numero com maior quantidade vendida?\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The number with the highest quantity sold is 27254 with a quantity of 94.'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opção 4 - Usando o PythonREPL\n",
    "\n",
    "https://python.langchain.com/docs/integrations/tools/python/\n",
    "\n",
    "Solução mais completa, incluir outras tools com as contas já pré-prontas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run(\"print(1+1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importante ter no prompt template:\n",
    "\n",
    "{input} – a pergunta do usuário\n",
    "\n",
    "{tools} – lista descritiva das ferramentas disponíveis\n",
    "\n",
    "{tool_names} – apenas os nomes das ferramentas, separados por vírgula\n",
    "\n",
    "{agent_scratchpad} – espaço para o agente mostrar seu raciocínio e ações anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Numero</th>\n",
       "      <th>Codigo do produto</th>\n",
       "      <th>Quantidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27192</td>\n",
       "      <td>1022450</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27227</td>\n",
       "      <td>1022450</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27235</td>\n",
       "      <td>1022450</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27250</td>\n",
       "      <td>1022450</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27254</td>\n",
       "      <td>1022450</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27265</td>\n",
       "      <td>1022450</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27266</td>\n",
       "      <td>1022450</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27269</td>\n",
       "      <td>1022450</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27281</td>\n",
       "      <td>1022450</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27283</td>\n",
       "      <td>1022450</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Numero  Codigo do produto  Quantidade\n",
       "0   27192            1022450          68\n",
       "1   27227            1022450          72\n",
       "2   27235            1022450          30\n",
       "3   27250            1022450          26\n",
       "4   27254            1022450          94\n",
       "5   27265            1022450          58\n",
       "6   27266            1022450          54\n",
       "7   27269            1022450          49\n",
       "8   27281            1022450          46\n",
       "9   27283            1022450          22"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tool_names = nomes das tools, lista com os nomes definidos, usados em Action: para escolher a tool\n",
    "\n",
    "tools = texto com nome + descrição, para informar ao modelo sobre as tools disponíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import OpenAI # Example LLM\n",
    "\n",
    "# Assuming df is your loaded Pandas DataFrame\n",
    "def query_excel_data(query: str) -> str:\n",
    "    \"\"\"A Python shell. Use this to execute python commands on the 'df' DataFrame.\n",
    "    When you want to see the output of a value, you should print it out with `print(...)`.\"\"\"\n",
    "    try:\n",
    "        # This is a simplified example; a real implementation would involve careful parsing\n",
    "        # and execution within a secure environment.\n",
    "        exec_globals = {'df': df, 'print': print}\n",
    "        exec(query, {}, exec_globals)\n",
    "        return \"Command executed successfully.\" # Or capture and return printed output\n",
    "    except Exception as e:\n",
    "        return f\"Error executing command: {e}\"\n",
    "\n",
    "#Definição da tool\n",
    "excel_query_tool = Tool(\n",
    "    name=\"Python_REPL\",\n",
    "    func=query_excel_data,\n",
    "    description=\"A Python shell. Use this to execute python commands on the 'df' DataFrame. When you want to see the output of a value, you should print it out with `print(...)`.\"\n",
    ")\n",
    "\n",
    "#Posso criar outras tools e incluir tudo nessa lista\n",
    "tool_names = [excel_query_tool]\n",
    "\n",
    "# ... (define LLM and prompt template)\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "colunas_info = \"\"\"\n",
    "O DataFrame `df` tem as seguintes colunas:\n",
    "- Número (int) : número do produto\n",
    "- Codigo do produto (int): código de identificação do produto\n",
    "- Quantidade (inteiro): quantidade vendida\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "template = f\"\"\"Você é um assistente de dados que pode usar ferramentas para responder perguntas sobre um DataFrame do pandas chamado `df`.\n",
    "\n",
    "O `df` tem as seguintes colunas:\n",
    "{colunas_info}\n",
    "\n",
    "Você tem acesso às seguintes ferramentas:\n",
    "\n",
    "{{tool_names}}\n",
    "\n",
    "Use o seguinte formato:\n",
    "\n",
    "Question: a pergunta que você deve responder\n",
    "Thought: pense sobre o que fazer\n",
    "Action: a ação a ser tomada, deve ser uma das [{{tools}}]\n",
    "Action Input: o input para a ação\n",
    "Observation: o resultado da ação\n",
    "... (pode repetir Thought/Action/Action Input/Observation quantas vezes quiser)\n",
    "Thought: agora que sei a resposta\n",
    "Final Answer: a resposta final à pergunta original\n",
    "\n",
    "Comece!\n",
    "\n",
    "Question: {{input}}\n",
    "{{agent_scratchpad}}\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(llm, tools=tool_names, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Thought: Para encontrar o código de produto com maior quantidade vendida, primeiro preciso encontrar o maior valor em Quantidade. Em seguida, devo filtrar a linha no DataFrame onde a quantidade seja igual ao máximo encontrado.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: `max_quantity = df['Quantidade'].max()`\u001b[0m\u001b[36;1m\u001b[1;3mError executing command: invalid syntax (<string>, line 1)\u001b[0m\u001b[32;1m\u001b[1;3m O erro está ocorrendo porque eu não especifiquei que estou trabalhando com a coluna 'Quantidade'. Agora vamos especificar isso e continuaremos.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: `max_quantity = df['Quantidade'].max()`\u001b[0m\u001b[36;1m\u001b[1;3mError executing command: invalid syntax (<string>, line 1)\u001b[0m\u001b[32;1m\u001b[1;3m O erro está acontecendo porque o Python REPL está esperando uma única linha de código. Vamos dividir o código em duas partes separadas.\n",
      "\n",
      "Action: Python_REPL (Parte 1)\n",
      "Action Input: `max_quantity = df['Quantidade'].max()`\u001b[0mPython_REPL (Parte 1) is not a valid tool, try one of [Python_REPL].\u001b[32;1m\u001b[1;3m Oops! It seems I made a mistake. Let's continue the command in two separate parts as you suggested.\n",
      "\n",
      "Action: Python_REPL (Parte 1)\n",
      "Action Input: `max_quantity = df['Quantidade'].max()`\u001b[0mPython_REPL (Parte 1) is not a valid tool, try one of [Python_REPL].\u001b[32;1m\u001b[1;3m I apologize for the confusion earlier. Let's continue by breaking down the command into two parts as suggested.\n",
      "\n",
      "Action: Python_REPL (Parte 1)\n",
      "Action Input: `max_quantity = df['Quantidade'].max()`\u001b[0mPython_REPL (Parte 1) is not a valid tool, try one of [Python_REPL].\u001b[32;1m\u001b[1;3m I apologize for the confusion earlier. Let's continue by breaking down the command into two parts as suggested.\n",
      "\n",
      "Action: Python_REPL (Parte 1)\n",
      "Action Input: `max_quantity = df['Quantidade'].max()`\u001b[0mPython_REPL (Parte 1) is not a valid tool, try one of [Python_REPL].\u001b[32;1m\u001b[1;3m It seems there was a misunderstanding regarding the usage of Python_REPL. Python_REPL is a tool for executing Python commands interactively, but in this case, we need to execute multiple lines of code. Let's use a more suitable tool for defining and running complete scripts:\n",
      "\n",
      "Action: Python Script\n",
      "Action Input: (Define a new python script with the following content)\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Load DataFrame 'df' from your source here...\n",
      "max_quantity = df['Quantidade'].max()\n",
      "product_code = df[df['Quantidade'] == max_quantity]['Codigo do produto'].iloc[0]\n",
      "print(product_code)\n",
      "```\u001b[0mPython Script is not a valid tool, try one of [Python_REPL].\u001b[32;1m\u001b[1;3m It seems there was a misunderstanding regarding the tools available. In this case, we can write a simple Python script to find the answer instead of using an interactive Python shell (Python_REPL). Here's a Python script that should work for finding the product code with the maximum quantity sold:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Load DataFrame 'df' from your source here...\n",
      "max_quantity = df['Quantidade'].max()\n",
      "product_code = df[df['Quantidade'] == max_quantity]['Codigo do produto'].iloc[0]\n",
      "print(product_code)\n",
      "```\n",
      "\n",
      "After you load the DataFrame 'df' in your script, this code should work as expected: it finds the maximum quantity sold and then retrieves the corresponding product code. The output of this script will be the product code with the maximum quantity sold.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3m Action: Python Script (Continued)\n",
      "Action Input: Run the provided Python script to find the product code with the maximum quantity sold and print it.\n",
      "\n",
      "The output of this script will be the product code with the maximum quantity sold.\n",
      "\u001b[0mPython Script (Continued) is not a valid tool, try one of [Python_REPL].\u001b[32;1m\u001b[1;3m It seems there was an oversight in the previous response. To execute the provided Python script, I'll break it down into parts and provide you with the results for each part separately using Python_REPL. Here's the breakdown:\n",
      "\n",
      "1. First, let's load the DataFrame 'df'. For this example, I'll use a small dataset. You should replace it with your actual DataFrame.\n",
      "\n",
      "```python\n",
      "df = pd.DataFrame({\n",
      "    \"Número\": [1, 2, 3],\n",
      "    \"Codigo do produto\": [101, 102, 103],\n",
      "    \"Quantidade\": [5, 7, 4]\n",
      "})\n",
      "```\n",
      "\n",
      "2. After loading the DataFrame, we can find the maximum quantity sold:\n",
      "\n",
      "```python_REPL(Parte 1)\n",
      "Action Input: `max_quantity = df['Quantidade'].max()`\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3m Action: Python_REPL (Parte 1)\n",
      "Action Input: `max_quantity = df['Quantidade'].max()`\u001b[0mPython_REPL (Parte 1) is not a valid tool, try one of [Python_REPL].\u001b[32;1m\u001b[1;3m It seems there was an oversight in the previous response. To execute the provided Python script, I'll break it down into parts and provide you with the results for each part separately using Python_REPL. Here's the breakdown:\n",
      "\n",
      "1. First, let's load the DataFrame 'df'. For this example, I'll use a small dataset. You should replace it with your actual DataFrame.\n",
      "\n",
      "```python\n",
      "df = pd.DataFrame({\n",
      "    \"Número\": [1, 2, 3],\n",
      "    \"Codigo do produto\": [101, 102, 103],\n",
      "    \"Quantidade\": [5, 7, 4]\n",
      "})\n",
      "```\n",
      "\n",
      "2. After loading the DataFrame, we can find the maximum quantity sold:\n",
      "\n",
      "```python_REPL(Parte 1)\n",
      "Action Input: `max_quantity = df['Quantidade'].max()`\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'\u001b[32;1m\u001b[1;3m Action: Python_REPL (Parte 1)\n",
      "Action Input: `max_quantity = df['Quantidade'].max()`\u001b[0mPython_REPL (Parte 1) is not a valid tool, try one of [Python_REPL].\u001b[32;1m\u001b[1;3m I apologize for the confusion earlier. Let's continue by breaking down the command into two parts as suggested. In this case, we can write a simple Python script to find the answer instead of using an interactive Python shell (Python_REPL). Here's a Python script that should work for finding the product code with the maximum quantity sold:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Load DataFrame 'df' from your source here...\n",
      "max_quantity = df['Quantidade'].max()\n",
      "product_code = df[df['Quantidade'] == max_quantity]['Codigo do produto'].iloc[0]\n",
      "print(product_code)\n",
      "```\n",
      "\n",
      "After you load the DataFrame 'df' in your script, this code should work as expected: it finds the maximum quantity sold and then retrieves the corresponding product code. The output of this script will be the product code with the maximum quantity sold.\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:'"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[162]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQual o código de itentificação com maior quantidade vendida?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    164\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    170\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    171\u001b[39m         inputs,\n\u001b[32m    172\u001b[39m         outputs,\n\u001b[32m    173\u001b[39m         return_only_outputs,\n\u001b[32m    174\u001b[39m     )\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain\\agents\\agent.py:1625\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1624\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1625\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1632\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1633\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1634\u001b[39m             next_step_output,\n\u001b[32m   1635\u001b[39m             intermediate_steps,\n\u001b[32m   1636\u001b[39m             run_manager=run_manager,\n\u001b[32m   1637\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain\\agents\\agent.py:1325\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1318\u001b[39m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1322\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1323\u001b[39m ) -> Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m   1324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m         \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1334\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain\\agents\\agent.py:1352\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1349\u001b[39m     intermediate_steps = \u001b[38;5;28mself\u001b[39m._prepare_intermediate_steps(intermediate_steps)\n\u001b[32m   1351\u001b[39m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1352\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_action_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain\\agents\\agent.py:455\u001b[39m, in \u001b[36mRunnableAgent.plan\u001b[39m\u001b[34m(self, intermediate_steps, callbacks, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m final_output: Any = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_runnable:\n\u001b[32m    449\u001b[39m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[32m    450\u001b[39m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunnable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3437\u001b[39m, in \u001b[36mRunnableSequence.stream\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3430\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   3431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstream\u001b[39m(\n\u001b[32m   3432\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3435\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   3436\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m3437\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3423\u001b[39m, in \u001b[36mRunnableSequence.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3416\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\n\u001b[32m   3418\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3421\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   3422\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m3423\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_stream_with_config(\n\u001b[32m   3424\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3425\u001b[39m         \u001b[38;5;28mself\u001b[39m._transform,\n\u001b[32m   3426\u001b[39m         patch_config(config, run_name=(config \u001b[38;5;129;01mor\u001b[39;00m {}).get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name),\n\u001b[32m   3427\u001b[39m         **kwargs,\n\u001b[32m   3428\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2214\u001b[39m, in \u001b[36mRunnable._transform_stream_with_config\u001b[39m\u001b[34m(self, inputs, transformer, config, run_type, **kwargs)\u001b[39m\n\u001b[32m   2212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2213\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2214\u001b[39m         chunk: Output = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2215\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m   2216\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3385\u001b[39m, in \u001b[36mRunnableSequence._transform\u001b[39m\u001b[34m(self, inputs, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   3382\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3383\u001b[39m         final_pipeline = step.transform(final_pipeline, config)\n\u001b[32m-> \u001b[39m\u001b[32m3385\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1428\u001b[39m, in \u001b[36mRunnable.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1425\u001b[39m final: Input\n\u001b[32m   1426\u001b[39m got_first_val = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43michunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   1429\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The default implementation of transform is to buffer input and\u001b[39;49;00m\n\u001b[32m   1430\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# then call stream.\u001b[39;49;00m\n\u001b[32m   1431\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# It'll attempt to gather all input into a single chunk using\u001b[39;49;00m\n\u001b[32m   1432\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# the `+` operator.\u001b[39;49;00m\n\u001b[32m   1433\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If the input is not addable, then we'll assume that we can\u001b[39;49;00m\n\u001b[32m   1434\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# only operate on the last chunk,\u001b[39;49;00m\n\u001b[32m   1435\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# and we'll iterate until we get to the last chunk.\u001b[39;49;00m\n\u001b[32m   1436\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgot_first_val\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43michunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5650\u001b[39m, in \u001b[36mRunnableBindingBase.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5643\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5644\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\n\u001b[32m   5645\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5648\u001b[39m     **kwargs: Any,\n\u001b[32m   5649\u001b[39m ) -> Iterator[Output]:\n\u001b[32m-> \u001b[39m\u001b[32m5650\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.transform(\n\u001b[32m   5651\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5652\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5653\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5654\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1446\u001b[39m, in \u001b[36mRunnable.transform\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   1443\u001b[39m             final = ichunk\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[32m-> \u001b[39m\u001b[32m1446\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(final, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:522\u001b[39m, in \u001b[36mBaseChatModel.stream\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    520\u001b[39m input_messages = _normalize_messages(messages)\n\u001b[32m    521\u001b[39m run_id = \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m.join((_LC_ID_PREFIX, \u001b[38;5;28mstr\u001b[39m(run_manager.run_id)))\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:349\u001b[39m, in \u001b[36mChatOllama._stream\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_stream\u001b[39m(\n\u001b[32m    342\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    343\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    346\u001b[39m     **kwargs: Any,\n\u001b[32m    347\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m                \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_community\\chat_models\\ollama.py:194\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_chat_stream\u001b[39m(\n\u001b[32m    185\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    186\u001b[39m     messages: List[BaseMessage],\n\u001b[32m    187\u001b[39m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    188\u001b[39m     **kwargs: Any,\n\u001b[32m    189\u001b[39m ) -> Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    190\u001b[39m     payload = {\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._convert_messages_to_ollama_messages(messages),\n\u001b[32m    193\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_stream(\n\u001b[32m    195\u001b[39m         payload=payload, stop=stop, api_url=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/api/chat\u001b[39m\u001b[33m\"\u001b[39m, **kwargs\n\u001b[32m    196\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\requests\\models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\requests\\utils.py:562\u001b[39m, in \u001b[36mstream_decode_response_unicode\u001b[39m\u001b[34m(iterator, r)\u001b[39m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    561\u001b[39m decoder = codecs.getincrementaldecoder(r.encoding)(errors=\u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\urllib3\\response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\urllib3\\response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\urllib3\\response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "agent_executor.invoke({\"input\": \"Qual o código de itentificação com maior quantidade vendida?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## # Opção 4 - Usando o PythonREPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando a construção da query da execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI  # ou outro LLM de sua escolha\n",
    "\n",
    "# Seu DataFrame pandas (exemplo)\n",
    "import pandas as pd\n",
    "data = {\n",
    "    'Número': [1, 2, 3],\n",
    "    'Codigo do produto': [101, 102, 103],\n",
    "    'Quantidade': [10, 25, 7],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tool 1: gera a query Python que será executada\n",
    "def generate_query(user_input: str) -> str:\n",
    "    # Aqui você pode usar LLM para gerar a query, mas para simplificar:\n",
    "    # Apenas retorna um código Python que responde a pergunta do usuário.\n",
    "    # Exemplo simples: para \"qual produto mais vendido?\"\n",
    "    if \"produto mais vendido\" in user_input.lower():\n",
    "        return \"df.loc[df['Quantidade'].idxmax(), 'Codigo do produto']\"\n",
    "    return \"None\"\n",
    "\n",
    "query_generator_tool = Tool(\n",
    "    name=\"QueryGenerator\",\n",
    "    func=generate_query,\n",
    "    description=\"Gera uma query Python para analisar o DataFrame df com base na pergunta do usuário.\"\n",
    ")\n",
    "\n",
    "# Tool 2: executa a query Python e retorna o resultado\n",
    "def execute_query(query: str) -> str:\n",
    "    try:\n",
    "        local_vars = {}\n",
    "        global_vars = {\"df\": df}\n",
    "        # executa e armazena o resultado\n",
    "        result = eval(query, global_vars, local_vars)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao executar query: {e}\"\n",
    "\n",
    "python_executor_tool = Tool(\n",
    "    name=\"PythonExecutor\",\n",
    "    func=execute_query,\n",
    "    description=\"Executa uma query Python passada como string e retorna o resultado.\"\n",
    ")\n",
    "\n",
    "tools = [query_generator_tool, python_executor_tool]\n",
    "\n",
    "# Prompt template para o agente ReAct\n",
    "template = \"\"\"\n",
    "Você é um assistente que responde perguntas sobre um DataFrame pandas chamado df.\n",
    "\n",
    "Você pode usar duas ferramentas:\n",
    "\n",
    "- QueryGenerator: cria o código Python necessário para responder a pergunta.\n",
    "- PythonExecutor: executa o código Python gerado e retorna o resultado.\n",
    "\n",
    "Use o seguinte formato:\n",
    "\n",
    "Question: {input}\n",
    "Thought: Devo primeiro gerar a query, depois executar a query.\n",
    "Action: nome da ferramenta (QueryGenerator ou PythonExecutor)\n",
    "Action Input: input para a ferramenta\n",
    "Observation: resultado da ação\n",
    "... (pode repetir Thought/Action/Action Input/Observation quantas vezes quiser)\n",
    "Thought: agora sei a resposta\n",
    "Final Answer: resposta para o usuário\n",
    "\n",
    "Comece!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Inicializa o LLM (aqui uso ChatOpenAI, mas pode ser outro)\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# Cria o agente\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "# Exemplo de uso\n",
    "resposta = agent_executor.invoke({\"input\": \"Qual o código do produto mais vendido?\"})\n",
    "print(resposta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exemplo com mais de uma tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import pandas as pd\n",
    "\n",
    "# Exemplo de DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Código do produto': [101, 102, 103],\n",
    "    'Quantidade': [10, 25, 7],\n",
    "})\n",
    "\n",
    "# Tool 1: Gera uma query Python com base na pergunta\n",
    "def generate_query(user_input: str) -> str:\n",
    "    if \"produto mais vendido\" in user_input.lower():\n",
    "        return \"df.loc[df['Quantidade'].idxmax(), 'Código do produto']\"\n",
    "    elif \"quantidade total\" in user_input.lower():\n",
    "        return \"df['Quantidade'].sum()\"\n",
    "    else:\n",
    "        return \"df.head()\"\n",
    "\n",
    "query_generator_tool = Tool(\n",
    "    name=\"QueryGenerator\",\n",
    "    func=generate_query,\n",
    "    description=\"Gera uma string de código Python para analisar o DataFrame df com base na pergunta do usuário.\"\n",
    ")\n",
    "\n",
    "# Tool 2: Retorna diretamente uma query pronta para pegar a maior quantidade\n",
    "def max_quantity_query(_: str) -> str:\n",
    "    return \"df['Quantidade'].max()\"\n",
    "\n",
    "max_quantity_tool = Tool(\n",
    "    name=\"MaxQuantityTool\",\n",
    "    func=max_quantity_query,\n",
    "    description=\"Retorna a query df['Quantidade'].max(). Use quando o usuário perguntar sobre a maior quantidade vendida.\"\n",
    ")\n",
    "\n",
    "# Tool 3: Executa a string de código retornada por uma das duas tools anteriores\n",
    "def execute_query(query: str) -> str:\n",
    "    try:\n",
    "        result = eval(query, {\"df\": df})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao executar: {e}\"\n",
    "\n",
    "python_executor_tool = Tool(\n",
    "    name=\"PythonExecutor\",\n",
    "    func=execute_query,\n",
    "    description=\"Executa a query Python gerada pelas outras tools. Recebe uma string de código e retorna o resultado.\"\n",
    ")\n",
    "\n",
    "tools = [query_generator_tool, max_quantity_tool, python_executor_tool]\n",
    "\n",
    "# Prompt ReAct customizado para forçar o fluxo de 2 etapas\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Você é um assistente que responde perguntas sobre um DataFrame pandas chamado df.\n",
    "\n",
    "Etapas:\n",
    "1. Primeiro, escolha entre gerar uma query com QueryGenerator ou usar a query pronta com MaxQuantityTool.\n",
    "2. Depois, execute essa query usando a ferramenta PythonExecutor.\n",
    "3. Finalmente, retorne a resposta ao usuário.\n",
    "\n",
    "Use o seguinte formato:\n",
    "\n",
    "Question: {input}\n",
    "Thought: preciso decidir qual ferramenta usar.\n",
    "Action: nome da ferramenta (QueryGenerator ou MaxQuantityTool)\n",
    "Action Input: o texto da pergunta\n",
    "Observation: string com o código Python\n",
    "Thought: agora vou executar esse código para obter o resultado\n",
    "Action: PythonExecutor\n",
    "Action Input: código Python gerado\n",
    "Observation: resultado da execução\n",
    "Thought: agora sei a resposta\n",
    "Final Answer: resposta para o usuário\n",
    "\n",
    "Comece!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "# Teste 1 – ativa a Tool direta (MaxQuantityTool)\n",
    "resposta1 = agent_executor.invoke({\"input\": \"Qual a maior quantidade vendida?\"})\n",
    "print(\"\\nResposta 1:\", resposta1)\n",
    "\n",
    "# Teste 2 – ativa o gerador de query (QueryGenerator)\n",
    "resposta2 = agent_executor.invoke({\"input\": \"Qual o código do produto mais vendido?\"})\n",
    "print(\"\\nResposta 2:\", resposta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando com orquestrador + executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Etapa 1: Roteador (decide qual tool usar)\n",
    "route_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Dada a pergunta abaixo, diga se deve usar a ferramenta 'gerar_query' ou 'query_direta':\n",
    "{input}\n",
    "Ferramenta:\"\"\")\n",
    "\n",
    "router_chain = LLMChain(llm=llm, prompt=route_prompt)\n",
    "\n",
    "# Etapa 2: gerar ou escolher a query\n",
    "def get_query(route: str, user_input: str) -> str:\n",
    "    if \"direta\" in route:\n",
    "        return \"df['Quantidade'].max()\"\n",
    "    else:\n",
    "        # Aqui pode chamar um LLM ou heurística que gera a query\n",
    "        if \"produto mais vendido\" in user_input.lower():\n",
    "            return \"df.loc[df['Quantidade'].idxmax(), 'Código do produto']\"\n",
    "        else:\n",
    "            return \"df.head()\"\n",
    "\n",
    "# Etapa 3: executar\n",
    "def execute_query(query, df):\n",
    "    return eval(query, {\"df\": df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: df['Quantidade'].max()\n",
      "Resultado: 25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemplo de uso\n",
    "user_input = \"Qual a maior quantidade vendida?\"\n",
    "df = pd.DataFrame({'Código do produto': [101, 102], 'Quantidade': [10, 25]})\n",
    "\n",
    "route = router_chain.run(input=user_input)\n",
    "query = get_query(route, user_input)\n",
    "result = execute_query(query, df)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Resultado:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: df['Quantidade'].max()\n",
      "Resultado: 25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemplo de uso\n",
    "user_input = \"Qual o produto mais vendido?\"\n",
    "df = pd.DataFrame({'Código do produto': [101, 102], 'Quantidade': [10, 25]})\n",
    "\n",
    "route = router_chain.run(input=user_input)\n",
    "query = get_query(route, user_input)\n",
    "result = execute_query(query, df)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Resultado:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# + uma variação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Qual foi o produto mais vendido?\n",
      "Resposta: 25\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, AgentExecutor\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import pandas as pd\n",
    "\n",
    "# Simula o DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Produto': ['A', 'B', 'C'],\n",
    "    'Quantidade': [10, 25, 15]\n",
    "})\n",
    "\n",
    "# Tool 1: cálculo direto\n",
    "def max_quantidade_tool(_):\n",
    "    return df['Quantidade'].max()\n",
    "\n",
    "# Tool 2: gera uma query com base na pergunta (via LLM)\n",
    "def gerar_query_llm(pergunta):\n",
    "    if \"produto mais vendido\" in pergunta.lower():\n",
    "        return \"df.loc[df['Quantidade'].idxmax(), 'Produto']\"\n",
    "    elif \"quantidade total\" in pergunta.lower():\n",
    "        return \"df['Quantidade'].sum()\"\n",
    "    else:\n",
    "        return \"df.head()\"\n",
    "\n",
    "# Tool 3: executa o código Python\n",
    "def executar_query(codigo_python):\n",
    "    try:\n",
    "        return str(eval(codigo_python, {\"df\": df}))\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao executar: {e}\"\n",
    "\n",
    "# LLM para roteamento\n",
    "\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# Roteador (decide qual caminho seguir)\n",
    "roteador_template = PromptTemplate.from_template(\n",
    "    \"\"\"Você é um roteador de ferramentas.\n",
    "Com base na pergunta abaixo, escolha apenas entre as opções:\n",
    "- 'direto' → se a pergunta se encaixa em um cálculo direto (por exemplo, 'quantidade máxima')\n",
    "- 'query' → se precisa interpretar e gerar código Python para ser executado depois\n",
    "\n",
    "Pergunta: {pergunta}\n",
    "Resposta:\"\"\"\n",
    ")\n",
    "\n",
    "roteador_chain = LLMChain(llm=llm, prompt=roteador_template)\n",
    "\n",
    "def agente_com_roteador(pergunta):\n",
    "    rota = roteador_chain.run(pergunta).strip().lower().replace(\"'\", \"\").replace('\"', '')\n",
    "\n",
    "    if rota == \"direto\":\n",
    "        return max_quantidade_tool(pergunta)\n",
    "\n",
    "    elif rota == \"query\":\n",
    "        query = gerar_query_llm(pergunta)\n",
    "        return executar_query(query)\n",
    "\n",
    "    else:\n",
    "        return f\"Rota não reconhecida: '{rota}'\"\n",
    "\n",
    "\n",
    "# 🧪 Teste\n",
    "pergunta = \"Qual foi o produto mais vendido?\"\n",
    "resposta = agente_com_roteador(pergunta)\n",
    "print(f\"Pergunta: {pergunta}\\nResposta: {resposta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÚLTIMO TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "import pandas as pd\n",
    "\n",
    "# Seu dataframe\n",
    "df = pd.DataFrame({\n",
    "    \"Produto\": [\"A\", \"B\", \"C\"],\n",
    "    \"Quantidade\": [10, 25, 15]\n",
    "})\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# 1. ROTEADOR — decide se é 'query' ou 'direto'\n",
    "# roteador_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# Decida se a pergunta abaixo deve ser respondida com:\n",
    "\n",
    "# - 'direto': se é uma métrica fixa como máximo, mínimo, soma, etc.\n",
    "# - 'query': se for preciso gerar um código pandas para obter a resposta.\n",
    "\n",
    "# Pergunta: {pergunta}\n",
    "# Resposta (apenas 'query' ou 'direto'):\n",
    "# \"\"\")\n",
    "# roteador_chain = roteador_prompt | llm | StrOutputParser()\n",
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "schema = [ResponseSchema(name=\"rota\", description=\"Nome da rota que deve ser usada: 'query' ou 'max_tool'\")]\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Classifique a pergunta abaixo e retorne a rota correta.\\n\\nPergunta: {pergunta}\\n\\nRotas disponíveis: 'query', 'max_tool'\\n\\nRetorne no formato JSON.\",\n",
    "    input_variables=[\"pergunta\"]\n",
    ")\n",
    "\n",
    "roteador_chain = prompt | llm | parser\n",
    "\n",
    "# 2. TOOL DIRETA — já embute a lógica de cálculo\n",
    "def max_quantidade_tool(pergunta: str) -> str:\n",
    "    max_valor = df[\"Quantidade\"].max()\n",
    "    return f\"A maior quantidade vendida foi {max_valor}\"\n",
    "\n",
    "# 3. TOOL DE GERAÇÃO DE FÓRMULA\n",
    "gerar_formula_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Você é um especialista em pandas. Gere apenas uma linha de código em pandas que responde à pergunta abaixo.\n",
    "O DataFrame se chama `df` e tem as colunas: Produto (str), Quantidade (int)\n",
    "\n",
    "Pergunta: {pergunta}\n",
    "\n",
    "Código em pandas:\n",
    "\"\"\")\n",
    "gerar_formula_chain = LLMChain(prompt=gerar_formula_prompt, llm=llm)\n",
    "\n",
    "def gerar_query_llm(pergunta: str) -> str:\n",
    "    return gerar_formula_chain.invoke({\"pergunta\": pergunta})[\"text\"].strip()\n",
    "\n",
    "# 4. EXECUTOR FINAL — executa o código pandas gerado\n",
    "def executar_query(query: str) -> str:\n",
    "    try:\n",
    "        local_vars = {\"df\": df}\n",
    "        exec(f\"resultado = {query}\", {}, local_vars)\n",
    "        return f\"Resposta: {local_vars['resultado']}\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao executar a query: {e}\"\n",
    "\n",
    "# 5. AGENTE FINAL — junta tudo\n",
    "def agente_com_roteador(pergunta):\n",
    "    rota_dict = roteador_chain.invoke({\"pergunta\": pergunta})\n",
    "    rota = rota_dict[\"rota\"].lower().strip()\n",
    "\n",
    "    if rota == \"max_tool\":\n",
    "        resposta = max_quantidade_tool(pergunta)\n",
    "    elif rota == \"query\":\n",
    "        formula = gerar_query_llm(pergunta)\n",
    "        resposta = executar_query(formula)\n",
    "    else:\n",
    "        resposta = f\"Rota não reconhecida: '{rota}'\"\n",
    "\n",
    "    print(resposta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid return object. Expected key `rota` to be present, but got {'rotas': ['query'], 'resposta': 'A rota correta para responder à pergunta é query.'}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[187]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43magente_com_roteador\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQual o produto mais vendido?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# → Resposta: A\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[186]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36magente_com_roteador\u001b[39m\u001b[34m(pergunta)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magente_com_roteador\u001b[39m(pergunta):\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     rota_dict = \u001b[43mroteador_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpergunta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpergunta\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     rota = rota_dict[\u001b[33m\"\u001b[39m\u001b[33mrota\u001b[39m\u001b[33m\"\u001b[39m].lower().strip()\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rota == \u001b[33m\"\u001b[39m\u001b[33mmax_tool\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3046\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3044\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3045\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3046\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3047\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3048\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:196\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1939\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1935\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1936\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1937\u001b[39m         output = cast(\n\u001b[32m   1938\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1939\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1947\u001b[39m         )\n\u001b[32m   1948\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1949\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:197\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    200\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    201\u001b[39m             config,\n\u001b[32m    202\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m         )\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:250\u001b[39m, in \u001b[36mBaseOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: \u001b[38;5;28mlist\u001b[39m[Generation], *, partial: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> T:\n\u001b[32m    236\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[32m    237\u001b[39m \n\u001b[32m    238\u001b[39m \u001b[33;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m \u001b[33;03m        Structured output.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain\\output_parsers\\structured.py:102\u001b[39m, in \u001b[36mStructuredOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    101\u001b[39m     expected_keys = [rs.name \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_schemas]\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\letic\\Documents\\env_openai\\Lib\\site-packages\\langchain_core\\utils\\json.py:196\u001b[39m, in \u001b[36mparse_and_check_json_markdown\u001b[39m\u001b[34m(text, expected_keys)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_obj:\n\u001b[32m    192\u001b[39m         msg = (\n\u001b[32m    193\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot invalid return object. Expected key `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    194\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto be present, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    195\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m json_obj\n",
      "\u001b[31mOutputParserException\u001b[39m: Got invalid return object. Expected key `rota` to be present, but got {'rotas': ['query'], 'resposta': 'A rota correta para responder à pergunta é query.'}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "print(agente_com_roteador(\"Qual o produto mais vendido?\"))\n",
    "# → Resposta: A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando query: Para encontrar o produto mais vendido no DataFrame `df`, você pode usar a função `value_counts()` para contar a quantidade de cada produto e então retornar o primeiro índice, que corresponde ao produto mais vendido.\n",
      "\n",
      "```python\n",
      "df['Produto'].value_counts().index[0]\n",
      "```\n",
      "df['Produto'].value_counts().index[0]\n",
      "Resposta: A\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "import pandas as pd\n",
    "\n",
    "# Seu dataframe\n",
    "df = pd.DataFrame({\n",
    "    \"Produto\": [\"A\", \"B\", \"C\"],\n",
    "    \"Quantidade\": [10, 25, 15]\n",
    "})\n",
    "\n",
    "# Inicializa o LLM\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# Define o schema para o output parser do roteador\n",
    "schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"rota\",\n",
    "        description=\"Nome da rota a ser usada, que pode ser 'query' ou 'max_tool'\"\n",
    "    )\n",
    "]\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "\n",
    "# Prompt do roteador (com instrução clara para retornar só o JSON)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "# Escapa as chaves para não causar erro no PromptTemplate\n",
    "format_instructions_escaped = format_instructions.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "roteador_prompt = PromptTemplate(\n",
    "    input_variables=[\"pergunta\"],\n",
    "    template=(\n",
    "        \"Classifique a pergunta abaixo e retorne SOMENTE um JSON com a rota correta.\\n\\n\"\n",
    "        \"Pergunta: {pergunta}\\n\\n\"\n",
    "        \"Rotas disponíveis: 'query', 'max_tool'\\n\\n\"\n",
    "        \"Retorne no formato JSON:\\n\"\n",
    "        f\"{format_instructions_escaped}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Monta o chain do roteador com o parser\n",
    "roteador_chain = LLMChain(prompt=roteador_prompt, llm=llm, output_parser=parser)\n",
    "\n",
    "# Tool 1: cálculo direto (exemplo)\n",
    "def max_quantidade_tool(pergunta: str) -> str:\n",
    "    max_valor = df[\"Quantidade\"].max()\n",
    "    return f\"A maior quantidade vendida foi {max_valor}\"\n",
    "\n",
    "# Tool 2: gera código pandas para a pergunta (via LLM)\n",
    "gerar_formula_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Você é um especialista em pandas. Gere apenas uma linha de código pandas que responde à pergunta abaixo.\n",
    "O DataFrame se chama `df` e tem as colunas: Produto (str), Quantidade (int).\n",
    "\n",
    "Pergunta: {pergunta}\n",
    "\n",
    "Código em pandas:\n",
    "\"\"\")\n",
    "gerar_formula_chain = LLMChain(prompt=gerar_formula_prompt, llm=llm)\n",
    "\n",
    "def gerar_query_llm(pergunta: str) -> str:\n",
    "    return gerar_formula_chain.invoke({\"pergunta\": pergunta})[\"text\"].strip()\n",
    "\n",
    "\n",
    "import re\n",
    "#Função pra limpar o output da query gerado\n",
    "def extrair_codigo_python(texto: str) -> str:\n",
    "    padrao = r\"```python\\s*(.*?)```\"\n",
    "    resultado = re.search(padrao, texto, re.DOTALL)\n",
    "    if resultado:\n",
    "        return resultado.group(1).strip()\n",
    "    else:\n",
    "        # Se não encontrar, tenta pegar o texto todo (pode ajustar se quiser)\n",
    "        return texto.strip()\n",
    "    \n",
    "\n",
    "# Tool 3: executa o código Python no df\n",
    "def executar_query(query: str) -> str:\n",
    "    try:\n",
    "        print(f\"Executando query: {query}\")  # DEBUG\n",
    "        codigo_limpo = extrair_codigo_python(query)\n",
    "        print(codigo_limpo)\n",
    "        local_vars = {\"df\": df}\n",
    "        exec(f\"resultado = {codigo_limpo}\", {}, local_vars)\n",
    "        return f\"Resposta: {local_vars['resultado']}\"\n",
    "    except Exception as e:\n",
    "        return f\"Erro ao executar a query: {e}\"\n",
    "\n",
    "# Função principal que usa o roteador para decidir a rota\n",
    "def agente_com_roteador(pergunta: str):\n",
    "    rota_dict = roteador_chain.invoke({\"pergunta\": pergunta})\n",
    "    # rota = rota_dict[\"rota\"].lower().strip()\n",
    "    rota = rota_dict[\"text\"][\"rota\"].lower().strip()\n",
    "\n",
    "    if rota == \"max_tool\":\n",
    "        resposta = max_quantidade_tool(pergunta)\n",
    "    elif rota == \"query\":\n",
    "        formula = gerar_query_llm(pergunta)\n",
    "        resposta = executar_query(formula)\n",
    "    else:\n",
    "        resposta = f\"Rota não reconhecida: '{rota}'\"\n",
    "\n",
    "    print(resposta)\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    pergunta = \"Qual o produto mais vendido?\"\n",
    "    agente_com_roteador(pergunta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pergunta': 'Qual o produto mais vendido?', 'text': {'rota': 'query'}}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rota_dict = roteador_chain.invoke({\"pergunta\": pergunta})\n",
    "rota_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A maior quantidade vendida foi 25\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"Qual a maior quantidade vendida?\"\n",
    "agente_com_roteador(pergunta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo genérico escolhendo tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import ChatOllama\n",
    "\n",
    "# Exemplo LLM\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# Descrição das ferramentas (tools)\n",
    "tools_descriptions = {\n",
    "    \"max_tool\": \"Retorna o valor máximo da coluna 'Quantidade'.\",\n",
    "    \"query_tool\": \"Gera código pandas para consultas complexas.\",\n",
    "    \"sum_tool\": \"Calcula a soma de valores em uma coluna específica.\",\n",
    "    # ... imagine até 10 tools aqui\n",
    "}\n",
    "\n",
    "# Cria o texto com as descrições para o prompt\n",
    "tools_info_text = \"\\n\".join([f\"- {name}: {desc}\" for name, desc in tools_descriptions.items()])\n",
    "\n",
    "roteador_prompt = PromptTemplate(\n",
    "    input_variables=[\"pergunta\"],\n",
    "    template=f\"\"\"\n",
    "Você é um roteador inteligente que recebe uma pergunta e escolhe a melhor ferramenta para respondê-la.\n",
    "\n",
    "Ferramentas disponíveis:\n",
    "{tools_info_text}\n",
    "\n",
    "Pergunta: {{pergunta}}\n",
    "\n",
    "Qual o nome da ferramenta mais apropriada para responder essa pergunta?\n",
    "Retorne SOMENTE o nome exato da ferramenta (exemplo: max_tool)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "roteador_chain = LLMChain(prompt=roteador_prompt, llm=llm)\n",
    "\n",
    "# Função para obter a tool escolhida\n",
    "def escolher_tool(pergunta: str) -> str:\n",
    "    tool_nome = roteador_chain.invoke({\"pergunta\": pergunta}).strip()\n",
    "    return tool_nome\n",
    "\n",
    "# Exemplo de uso\n",
    "pergunta_exemplo = \"Qual o maior valor da coluna Quantidade?\"\n",
    "tool_escolhida = escolher_tool(pergunta_exemplo)\n",
    "print(\"Tool escolhida:\", tool_escolhida)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
